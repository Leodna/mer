{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from my_functions.my_fun import *\n",
    "from my_functions.my_models import *\n",
    "from my_functions.My_nn import My_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "ASSETS_DIR = os.getenv('ASSETS_DIR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar las etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar archivo de anotaciones y comprobar que existe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anot = os.getenv('PMEMO_ANOTATIONS')\n",
    "stat_anot_file = os.path.join(anot,'static_annotations.csv')\n",
    "\n",
    "if os.path.exists(stat_anot_file):\n",
    "    print(\"El archivo existe\")\n",
    "else:\n",
    "    print(\"No hay no existe\")\n",
    "    \n",
    "anotaciones_df = pd.read_csv(stat_anot_file)\n",
    "anotaciones_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = anotaciones_df[['Arousal(mean)','Valence(mean)']].values\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar los espectrogramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar los espectrogramas ajustados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrograms = load_spectrograms(filename='spectrograms_down_padd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrograms = np.array(spectrograms)\n",
    "print(type(spectrograms))\n",
    "print(type(spectrograms[0]))\n",
    "print(spectrograms[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrograms = np.expand_dims(spectrograms, axis=-1)\n",
    "print(spectrograms.shape)\n",
    "print(spectrograms[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividir los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamiento y pruebas\n",
    "X_train,X_test,y_train,y_test = train_test_split( spectrograms,\n",
    "                                                 labels,\n",
    "                                                 test_size=0.2,\n",
    "                                                 random_state=83)\n",
    "#Entrenamiento y validación\n",
    "X_train,X_val, y_train,y_val =  train_test_split( X_train,\n",
    "                                                  y_train,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asumiendo que `spectrograms_array` es tu array de espectrogramas\n",
    "print(\"Valor mínimo:\", np.min(spectrograms))\n",
    "print(\"Valor máximo:\", np.max(spectrograms))\n",
    "print(\"Media:\", np.mean(spectrograms))\n",
    "print(\"Desviación estándar:\", np.std(spectrograms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplana el array para facilitar la visualización\n",
    "values = spectrograms.flatten()\n",
    "\n",
    "plt.hist(values, bins=50)\n",
    "plt.title('Histograma de Valores de Espectrogramas')\n",
    "plt.xlabel('Valor')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener un resumen estadístico\n",
    "stats = {\n",
    "    'Mínimo': np.min(values),\n",
    "    'Máximo': np.max(values),\n",
    "    'Media': np.mean(values),\n",
    "    'Desviación Estándar': np.std(values),\n",
    "    '25º Percentil': np.percentile(values, 25),\n",
    "    '50º Percentil (Mediana)': np.percentile(values, 50),\n",
    "    '75º Percentil': np.percentile(values, 75)\n",
    "}\n",
    "\n",
    "for key, value in stats.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar los datos\n",
    "X_train_norm = X_train / np.max(X_train)\n",
    "X_val_norm = X_val / np.max(X_val)\n",
    "X_test_norm = X_test / np.max(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar el espectrograma de la primer canción\n",
    "aux_spec = np.squeeze(X_train[1])\n",
    "\n",
    "\n",
    "#Visualizar\n",
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "img = librosa.display.specshow(aux_spec,\n",
    "                               x_axis='time',\n",
    "                               y_axis='log',\n",
    "                               ax=ax)\n",
    "\n",
    "ax.set_title('Espectrograma de la primer canción',fontsize=(20))\n",
    "fig.colorbar(img,ax=ax,format=f'%0.2f')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar el espectrograma de la primer canción pero normalizada\n",
    "espectrograma_normalizado = np.squeeze(X_train_norm[1])\n",
    "# Visualizar el espectrograma normalizado\n",
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "img = librosa.display.specshow(espectrograma_normalizado,\n",
    "                               x_axis='time',\n",
    "                               y_axis='log',\n",
    "                               ax=ax)\n",
    "ax.set_title('Espectrograma de la primer canción normalizada',fontsize=(20))\n",
    "fig.colorbar(img, ax=ax, format=f'%0.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar los datos\n",
    "X_train = X_train_norm\n",
    "X_val = X_val_norm\n",
    "X_test = X_test_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardar datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los datos en un archivo .h5\n",
    "DATOS_DIR = os.path.join(ASSETS_DIR,'processed_data.h5')\n",
    "\n",
    "with h5py.File(DATOS_DIR, 'w') as hf:\n",
    "    hf.create_dataset('X_train', data=X_train)\n",
    "    hf.create_dataset('X_test', data=X_test)\n",
    "    hf.create_dataset('X_val', data=X_val)\n",
    "    hf.create_dataset('y_train', data=y_train)\n",
    "    hf.create_dataset('y_test', data=y_test)\n",
    "    hf.create_dataset('y_val', data=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "# Cargar los datos desde un archivo .h5\n",
    "DATOS_DIR = os.path.join(ASSETS_DIR,'processed_data.h5')\n",
    "with h5py.File(DATOS_DIR, 'r') as hf:\n",
    "    X_train = hf['X_train'][:]\n",
    "    X_test = hf['X_test'][:]\n",
    "    X_val = hf['X_val'][:]\n",
    "    y_train = hf['y_train'][:]\n",
    "    y_test = hf['y_test'][:]\n",
    "    y_val = hf['y_val'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------ENTRENAMIENTO--------------------------------\n",
      "Dimensión datos de entrenamiento: (490, 513, 1671, 1)\n",
      "Dimensión etiquetas de entrenamiento: (490, 2)\n",
      "--------------------PRUEBAS--------------------------------\n",
      "Dimensión datos de prueba: (154, 513, 1671, 1)\n",
      "Dimensión etiquetas de prueba: (154, 2)\n",
      "--------------------VALIDACIÓN--------------------------------\n",
      "Dimensión datos de validación: (123, 513, 1671, 1)\n",
      "Dimensión etiquetas de validación: (123, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f'--------------------ENTRENAMIENTO--------------------------------')\n",
    "print(f'Dimensión datos de entrenamiento: {X_train.shape}')\n",
    "print(f'Dimensión etiquetas de entrenamiento: {y_train.shape}')\n",
    "print(f'--------------------PRUEBAS--------------------------------')\n",
    "print(f'Dimensión datos de prueba: {X_test.shape}')\n",
    "print(f'Dimensión etiquetas de prueba: {y_test.shape}')\n",
    "print(f'--------------------VALIDACIÓN--------------------------------')\n",
    "print(f'Dimensión datos de validación: {X_val.shape}')\n",
    "print(f'Dimensión etiquetas de validación: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 513, 1671, 1)]    0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 513, 1671, 32)     320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 256, 835, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256, 835, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 256, 835, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 128, 417, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 128, 417, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 128, 417, 128)     73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 64, 208, 128)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 64, 208, 128)     512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 64, 208, 256)      295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 32, 104, 256)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 32, 104, 256)     1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 851968)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               109052032 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,450,178\n",
      "Trainable params: 109,449,218\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 513, 1671, 1)]    0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 513, 1671, 32)     320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 256, 835, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256, 835, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 256, 835, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 128, 417, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 128, 417, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 128, 417, 128)     73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 64, 208, 128)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 64, 208, 128)     512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 64, 208, 256)      295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 32, 104, 256)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 32, 104, 256)     1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 851968)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               109052032 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,450,178\n",
      "Trainable params: 109,449,218\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leo_d\\anaconda3\\envs\\mirtools\\lib\\site-packages\\visualkeras\\layered.py:86: UserWarning: The legend_text_spacing_offset parameter is deprecated and will be removed in a future release.\n",
      "  warnings.warn(\"The legend_text_spacing_offset parameter is deprecated and will be removed in a future release.\")\n"
     ]
    }
   ],
   "source": [
    "modelo1 = modelo1()\n",
    "primer_exp = My_nn(X_train=X_train,y_train=y_train,\n",
    "                   X_test=X_test,y_test=y_test,\n",
    "                   X_val=X_val,y_val=y_val,\n",
    "                   model=modelo1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'batchsize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hist \u001b[38;5;241m=\u001b[39m  \u001b[43mprimer_exp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\python_proy\\mer\\my_functions\\My_nn.py:37\u001b[0m, in \u001b[0;36mMy_nn.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 37\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatchsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[1;32mc:\\Users\\leo_d\\anaconda3\\envs\\mirtools\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\leo_d\\anaconda3\\envs\\mirtools\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'batchsize'"
     ]
    }
   ],
   "source": [
    "hist =  primer_exp.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mirtools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
